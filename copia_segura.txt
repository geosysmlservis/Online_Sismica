import os
import time
import json
import base64
import logging
import pandas as pd
from flask import Flask, request, jsonify
from google.cloud import storage, bigquery
from vertexai import init
from vertexai.preview.generative_models import GenerativeModel, Part
import vertexai.preview.generative_models as generative_models

# Configuración de credenciales
os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "credentialsinterno.json"

# Inicializar Vertex AI y logging
init(project="interno-davinci-analitica", location="us-central1")
logging.basicConfig(level=logging.INFO)

# Cargar listas
def load_csv_list(file_path):
    return pd.read_csv(file_path).iloc[:, 0].dropna().unique().tolist()

contratista = load_csv_list("contratista.csv")
operadora = load_csv_list("operadora.csv")
tipo_procesamiento = load_csv_list("tipo_procesamiento.csv")

# Prompt base
prompt_base = """
Eres un sistema experto en extracción de información estructurada desde documentos técnicos sísmicos. Tu objetivo es extraer con precisión los campos especificados y devolver un objeto JSON con los valores extraídos. Sigue todas las instrucciones al pie de la letra. Devuelve únicamente JSON válido. No expliques ni añadas comentarios.

                            INSTRUCCIONES GENERALES:

                            1. Analiza completamente el documento escaneado o imagen antes de comenzar.
                            2. Detecta zonas clave con información técnica: recuadros, encabezados, pies de página, tablas, márgenes o secciones etiquetadas.
                            3. Aplica correcciones comunes de OCR:
                            - Errores frecuentes: “O” ↔ “0”, “I” ↔ “1”, “B” ↔ “8”, “S” ↔ “5”, “Z” ↔ “2”, “3” ↔ “8”.
                            4. Si hay múltiples candidatos para un campo, prioriza:
                            - Cercanía a la etiqueta
                            - Alineación (a la derecha o en la misma línea de la etiqueta)
                            - Jerarquía visual
                            5. Usa formatos estandarizados:
                            - Fechas en AAAA-MM-DD
                            - Campos nulos como null (sin comillas)
                            6. Devuelve únicamente el objeto JSON especificado más abajo. Sin explicaciones adicionales.
                            7. Si un valor no cumple las reglas, asigna null.

                            CAMPOS A EXTRAER:

                            1. codigo:
                            - Debe comenzar con "SG" o "SGP" seguido de 4 a 6 dígitos.
                            - Rechaza códigos con guiones, espacios o letras en la parte numérica.
                            - Ubicación típica: cerca del logo, código de barras, encabezado o pie de página.
                            - Ejemplos válidos: SG00577, SGP21081
                            - Ejemplos inválidos: RIB93-26 → null

                            2. fecha:
                            - Etiquetas: "FECHA", "DATE", "PROCESSING DATE", "DISPLAY DATE"
                            - Formato: AAAA-MM-DD
                            - Si solo hay mes y año → usa día 01.
                            - Si el formato es ambiguo (ej. 05/06/97), analiza el contexto para determinar el orden.

                            3. datum:
                            - Etiquetas: “DATUM”, “DATUM LEVEL”, “DATUM ELEVATION”
                            - Suele estar alineado a la derecha de la etiqueta.
                            - Si es poco confiable → null

                            4. shot_point_range:
                            - Formato esperado: valor-valor
                            - Etiquetas: “SP”, “SHOT POINT”, “STA”
                            - Si no hay valor válido → null

                            5. nombre_de_linea:
                            - Etiquetas válidas: “LINE”, “LINE NAME”, “LÍNEA”, “LINE NO.” (no se aceptan abreviaciones)
                            - Extrae el valor inmediatamente junto a la etiqueta.
                            - Ignora líneas inferiores o valores no asociados directamente.

                            6. velocidad_de_reemplazamiento:
                            - Palabras clave: “Replacement velocity”, “Correctional velocity”
                            - Formato: número + unidad (ej. 2000 m/s)
                            - Si no se encuentra → null

                            7. dominio_profundidad:
                            - Busca en “PROCESSING SEQUENCE” o “Processing sequence”
                            - Si contiene la palabra “depth” → true, si no → false

                            8. intervalos_de_fuentes:
                            - Palabras clave: “shot interval”, “SP interval”, “Source interval”
                            - Formato: número + unidad (ej. 25 m)
                            - Si no se encuentra → null

                            9. intervalo_de_receptores:
                            - Palabras clave: “station interval”, “receiver interval”, “group interval”
                            - Formato: número + unidad
                            - Si no se encuentra → null

                            10. lote:
                            - Etiquetas: “BLOCK”, “LOTE”, “BLOCKS”
                            - Extrae directamente de la etiqueta
                            - Si es ambiguo o no aparece → null

                            11. station:
                            - Formato preferido: “200–51”
                            - Estrategia de extracción:
                                1. Etiqueta “STATION” con rango claro
                                2. Primera y última cifra de una tabla horizontal cerca de “STATION”
                                3. Ejes horizontales del gráfico sísmico (primero y último número)
                            - Si no se puede determinar → null

                            12. analisis_velocidades:
                            - true si:
                                - Hay múltiples tablas pequeñas con decimales
                                - O aparece la etiqueta “VELOCITY ANALYSIS”
                            - Si no → false

                            13. procesado_por:
                            - Usa la lista {contratistas_list}
                            - Coincidencia por nombre con la lista, ignorando mayúsculas/minúsculas y errores OCR menores
                            - Si no se encuentra → null

                            14. procesado_para:
                            - Usa la lista {operadoras_list}
                            - Reglas iguales a "procesado_por"

                            15. tipo_procesamiento:
                            - Usa la lista {tipos_procesamiento_list}
                            - Si no se encuentra coincidencia → null

                            FORMATO DE SALIDA:

                            Devuelve el siguiente objeto JSON (exactamente con esta estructura). No añadas comentarios ni texto adicional.

                            {{
                            "codigo": null,
                            "fecha": null,
                            "datum": null,
                            "shot_point_range": null,
                            "nombre_de_linea": null,
                            "velocidad_de_reemplazamiento": null,
                            "dominio_profundidad": null,
                            "intervalos_de_fuentes": null,
                            "intervalo_de_receptores": null,
                            "lote": null,
                            "station": null,
                            "analisis_velocidades": null,
                            "procesado_por": null,
                            "procesado_para": null,
                            "tipo_procesamiento": null
                            }}
"""

# Configurar modelo Gemini
generation_config = {
    "max_output_tokens": 8192,
    "temperature": 0,
    "top_p": 0.95,
}
safety_settings = {
    generative_models.HarmCategory.HARM_CATEGORY_HATE_SPEECH: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
    generative_models.HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
    generative_models.HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
    generative_models.HarmCategory.HARM_CATEGORY_HARASSMENT: generative_models.HarmBlockThreshold.BLOCK_ONLY_HIGH,
}

def build_prompt():
    return prompt_base.format(
        tipos_procesamiento_list=tipo_procesamiento,
        operadoras_list=operadora,
        contratistas_list=contratista
    )

def download_blob_as_bytes(bucket_name, blob_path):
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_path)
    return blob.download_as_bytes(), blob.content_type

def generate_from_document(document1, prompt):
    model = GenerativeModel("gemini-2.5-pro")
    responses = model.generate_content(
        [document1, prompt],
        generation_config=generation_config,
        safety_settings=safety_settings,
    )
    return responses.text.strip()

def save_to_bigquery(file_name, respuesta_texto):
    client = bigquery.Client()
    dataset_id = "geosys_batch_sismos"
    table_id = f"{client.project}.{dataset_id}.resultados"

    # Verificar si la tabla existe; si no, crearla
    try:
        client.get_table(table_id)
        logging.info(f"[BigQuery] La tabla existe: {table_id}")
    except Exception:
        logging.info(f"[BigQuery] Creando tabla: {table_id}")
        schema = [
            bigquery.SchemaField("archivo", "STRING"),
            bigquery.SchemaField("respuesta_modelo", "STRING"),
        ]
        table = bigquery.Table(table_id, schema=schema)
        client.create_table(table)

    # Verificar si ya existe un registro con ese archivo
    query = f"""
        SELECT COUNT(*) as total FROM {table_id}
        WHERE archivo = @archivo
    """
    job_config = bigquery.QueryJobConfig(
        query_parameters=[
            bigquery.ScalarQueryParameter("archivo", "STRING", file_name)
        ]
    )
    result = client.query(query, job_config=job_config).result()
    existe = next(result).total > 0

    if existe:
        logging.info(f"[BigQuery] Ya existe: {file_name}, actualizando...")
        update_query = f"""
            UPDATE {table_id}
            SET respuesta_modelo = @respuesta
            WHERE archivo = @archivo
        """
        update_config = bigquery.QueryJobConfig(
            query_parameters=[
                bigquery.ScalarQueryParameter("archivo", "STRING", file_name),
                bigquery.ScalarQueryParameter("respuesta", "STRING", respuesta_texto),
            ]
        )
        client.query(update_query, job_config=update_config).result()
    else:
        logging.info(f"[BigQuery] Insertando nuevo: {file_name}")
        rows_to_insert = [{"archivo": file_name, "respuesta_modelo": respuesta_texto}]
        errors = client.insert_rows_json(table_id, rows_to_insert)
        if errors:
            logging.error(f"[BigQuery] Error al insertar fila: {errors}")


# Flask App
app = Flask(__name__)

@app.route("/", methods=["GET"])
def health():
    return "API activa. Usa POST /extract_batch", 200

@app.route("/extract_batch", methods=["POST"])
def extract_batch():
    try:
        data = request.get_json(force=True)
        bucket_path = data.get("bucket_path")
        cantidad = int(data.get("cantidad", 1))

        if not bucket_path or not bucket_path.startswith("gs://"):
            return jsonify({"error": "bucket_path inválido"}), 400

        path_parts = bucket_path[5:].split("/", 1)
        bucket_name = path_parts[0]
        prefix = path_parts[1] if len(path_parts) > 1 else ""

        client = storage.Client()
        blobs = list(client.list_blobs(bucket_name, prefix=prefix))

        archivos_procesados = 0
        prompt = build_prompt()

        for blob in blobs:
            if archivos_procesados >= cantidad:
                break
            if blob.name.endswith((".pdf", ".jpg", ".png", ".tiff")):
                logging.info(f"[Procesando] {blob.name}")
                try:
                    file_bytes, mime_type = download_blob_as_bytes(bucket_name, blob.name)
                    part = Part.from_data(mime_type=mime_type, data=file_bytes)
                    respuesta = generate_from_document(part, prompt)
                    save_to_bigquery(blob.name, respuesta)
                    archivos_procesados += 1
                    time.sleep(1)  # espera entre cada archivo
                except Exception as e:
                    logging.exception(f"[ERROR] Al procesar {blob.name}: {e}")
                    continue

        return jsonify({"procesados": archivos_procesados}), 200

    except Exception as e:
        logging.exception("[Extract Batch] Error inesperado")
        return jsonify({"error": str(e)}), 500

if __name__ == "__main__":
    app.run(host="0.0.0.0", port=int(os.environ.get("PORT", 8080))) 